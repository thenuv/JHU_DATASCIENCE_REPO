---
title: "JHU Capstone NLP Project Milestone Report"
author: "thenuv"
date: "15 August 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

The final project for the Data science specialization is based on ***Text Analytics***. This milestone report is prepared to communicate the understanding of the data set, do some exploratory analysis, build a Corpus, slice a sample to preprocess and derive NGrams. This will be a base for fitting models and do predictions on the test data. 

___

## Import Data
         
### Download
        Data set is downloaded for this project from the URL provided. We set a seed and read the 3 files for text from Tweeter, Blog & News.

```{r ImportData, echo=TRUE, eval=TRUE}
# Load Libraries
suppressMessages(suppressWarnings(library(data.table)))
suppressMessages(suppressWarnings(library(quanteda)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caret)))

# Download Data
# fileurl1 <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
# filename1 <- "Coursera_SwiftKey.zip"
# 
# if (!file.exists(filename1)) {
#         download.file(fileurl1, filename1)
#         unzip(filename1)
# }

```

### Random Subset
        A sample of 5% is extracted with coin flip logic. The sample data is saved to file in order to reuse it latter.

```{r ImportData_2, echo=TRUE, eval=FALSE}
set.seed(1234)

# Read data from files downloaded
tweet_text <- readLines("en_US.twitter.txt", skipNul = TRUE)
tweet_text <- tweet_text[rbinom(length(tweet_text)*.05, length(tweet_text), 0.5)]

blog_text <- readLines("en_US.blogs.txt", skipNul = TRUE)
blog_text <- blog_text[rbinom(length(blog_text)*.05, length(blog_text), 0.5)]

news_text <- readLines("en_US.news.txt", skipNul = TRUE)
news_text <- news_text[rbinom(length(news_text)*.05, length(news_text), 0.5)]

# Write the samples to file
write.csv(tweet_text, file ="sample.tweet.txt", row.names = FALSE, col.names = NULL, quote = FALSE)
write.csv(blog_text, file ="sample.blog.txt", row.names = FALSE, col.names = NULL, quote = FALSE)
write.csv(news_text, file ="sample.news.txt", row.names = FALSE, col.names = NULL, quote = FALSE)

# Lines or Documents in each data set
c(length(tweet_text), length(blog_text), length(news_text))

```

### Read Data from the sample
        Re import the sample data saved to build the corpus.

```{r ImportData_3, echo=TRUE, eval=TRUE}

# Read data from Sample saved
tweet_text <- readLines("sample.tweet.txt")
blog_text <- readLines("sample.blog.txt")
news_text <- readLines("sample.news.txt")

tweet_text<- tweet_text[2:length(tweet_text)]
blog_text<- blog_text[2:length(blog_text)]
news_text<- news_text[2:length(news_text)]

df_txt1 <- data.frame(DataSource = as.factor("Tweeter"), Text = tweet_text)
df_txt2 <- data.frame(DataSource = as.factor("Blog"), Text = blog_text)
df_txt3 <- data.frame(DataSource = as.factor("News"), Text = news_text)
corp <- rbind(df_txt1,df_txt2,df_txt3)

rm(tweet_text, blog_text, news_text, df_txt1, df_txt2, df_txt3)

```

___

## Explore

```{r Explore, echo=TRUE, eval=TRUE }

dim(corp)                              # Observations in Sample
length(which(!complete.cases(corp)))   # Check for incomplete cases
corp$Text <- as.character(corp$Text)   # Convert to Char
corp$Length <- nchar(corp$Text)        # Add a column with Text length
prop.table(table(corp$DataSource))     # Look at the proporsition of data from different source

summary(corp$Length)
summary(corp$Length[corp$DataSource=="Tweeter"])
summary(corp$Length[corp$DataSource=="Blog"])
summary(corp$Length[corp$DataSource=="News"])


ggplot(corp, aes(x=Length, fill= DataSource)) +
        theme_bw() +
        geom_histogram(binwidth = 10) +
        coord_cartesian(xlim = c(0, 1000)) +
        labs(y= "Text Count", x= "Length of Text",
             title = "Distribution of Text length from Datasources")

```



___

## Preprocess Corpus
        A function is built for preprocessing. The following tasks are done.  
        
- Sample data is partitioned to create 2 set of 70% and 30% for Training and Testing.
- Non ASCII words are removed
- Tokenization
- Convert Case
- Remove Stop words
- Profanity Filtering
- Steming

```{r Preprocess, echo=TRUE }

set.seed(1234)

# Create Training & Test Data sets
idx <- createDataPartition(corp$DataSource, times = 1, p = 0.7, list = FALSE)
#idx <- createDataPartition(corp$DataSource, times = 1, p = 0.02, list = FALSE)
train <- corp[idx,]
test <- corp[-idx,]


# Remove non ASCII words
train$Text <- iconv(train$Text, from = "UTF-8", to = "ASCII" )
sum(is.na(train$Text))/nrow(train) #Proposition of non English words ~ 10%
train <- train[!(is.na(train$Text)),]


# Preprocess Corpus
preProcessCorpus <- function (corpusText) {
        # Tokenize text 
        text.tokens <- tokens(corpusText, 
                              what = "word", 
                              remove_punct = TRUE, 
                              remove_numbers = TRUE,
                              remove_symbols = TRUE,
                              remove_hyphens = TRUE)
        
        # convert to lower case
        text.tokens <- tokens_tolower(text.tokens)
        
        # Remove Stop words
        text.tokens <- tokens_select(text.tokens, stopwords(), selection = "remove")
        
        # Filter Profanity words
        prof_list <- read.csv("profanity_words.txt", header = FALSE)
        prof_list$V1 <- as.character(prof_list$V1)
        text.tokens <- tokens_select(text.tokens, prof_list$V1, selection = "remove")
        
        # Stemming
        text.tokens <- tokens_wordstem(text.tokens, language = "english")
        
} 

train.tokens <- preProcessCorpus(train$Text)

```


___
## Build NGRAMS

```{r ngram, eval=TRUE }
#NGrams
myDfm <- dfm(train.tokens, ngrams = 1, verbose = FALSE)
x <- textstat_frequency(myDfm)

myDfm2 <- dfm(train.tokens, ngrams = 2, verbose = FALSE)
y <- textstat_frequency(myDfm2)

myDfm3 <- dfm(train.tokens, ngrams = 3, verbose = FALSE)
z <- textstat_frequency(myDfm3)

g1 <- x[1:20,1:2]
g2 <- y[1:20,1:2]
g3 <- z[1:20,1:2]

p1 <- ggplot(g1, aes(x= reorder(feature, frequency), y=frequency)) +
        theme_bw() +
        geom_bar(stat="identity", width = .2)+
        labs(y= "Text Count", x= "Word",
             title = "1 GRAM") +
        coord_flip() 


p2 <- ggplot(g2, aes(x= reorder(feature, frequency), y=frequency)) +
        theme_bw() +
        geom_bar(stat="identity", width = .2)+
        labs(y= "Text Count", x= "Word",
             title = "2 GRAMS") +
        coord_flip() 


p3 <- ggplot(g3, aes(x= reorder(feature, frequency), y=frequency)) +
        theme_bw() +
        geom_bar(stat="identity", width = .2)+
        labs(y= "Text Count", x= "Word",
             title = "3 GRAMS") +
        coord_flip() 

library(gridExtra)
grid.arrange (p1, p2, p3, widths = c(1,1.3,1.7), nrow = 1, top = "Top 25 Frequent NGRAM")

myDfm
myDfm2
myDfm3
```
___
## Findings

- There about 15K features or words with 1gram out of 130K documents.
- 2grams and 3grams has 89K words.
- There were about 10% of non English words.

___
## Conclusion

The preprocessed corpus will be used for modeling the predictions in the project.
Katz Back Off model will be used to predict the next word. There will be a simple app build on Shiny to take user input for few word and the next word will be predicted with the model developed. The output would display the top 5 or 10 possible words with higher probability.

___

